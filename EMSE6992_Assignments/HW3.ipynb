{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "\n",
    "### Due by 11:59pm on 30 November ( submit within Portfolio )\n",
    "\n",
    "For each instruction, show your code and execution within the Jupyter Notebook.  Download example files from:\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics\n",
    " - files are located in the auto_examples_jupyter directory\n",
    "\n",
    "For HW3 we will be using scikit-learn. Follow the instructions and use the examples provided within the instruction that use the Iris dataset.\n",
    "\n",
    "Once complete, upload your results to Github and update the Assignment 3 link within your portfolio's.\n",
    "\n",
    "Instruction 1: Provide an example of concatenating multiple feature extraction methods using your dataset.\n",
    "\n",
    "In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection.\n",
    "Combining features using this transformer has the benefit that it allows cross validation and grid searches over the whole process.\n",
    "The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/plot_feature_stacker.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/plot_feature_stacker.ipynb\n",
    "    \n",
    "\n",
    "### Applications\n",
    "Instruction 2: Provide an example of Outlier detection on your dataset.\n",
    "\n",
    "This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.\n",
    "We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.\n",
    "In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/applications/plot_outlier_detection_housing.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/applications/plot_outlier_detection_housing.ipynb\n",
    "\n",
    "### Classification\n",
    "Instruction 3: Provide an example of Classifier Comparison using yoru dataset\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.\n",
    "Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\n",
    "The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n",
    "auto_examples_jupyter/Classification/plot_classifier_comparison. ipynb\n",
    "Plot classification probability\n",
    "Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting, and Gaussian process classification.\n",
    "The logistic regression is not a multiclass classifier out of the box. As a result it can identify only the first class.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/auto_examples_jupyter/Classification/plot_lda_qda.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/classification/plot_lda_qda.ipynb\n",
    "\n",
    "### Clustering\n",
    "Instruction 4: Provide an example of K-means Clustering using your dataset\n",
    "\n",
    "The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/clustering/plot_cluster_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/cluster/plot_cluster_iris.ipynb\n",
    "\n",
    "\n",
    "### Covariance Estimation\n",
    "Instruction 4: Provide an example of Outlier detection with covariance estimation using your dataset.\n",
    "\n",
    "When the amount of contamination is known, this example illustrates three different ways of performing Novelty and Outlier Detection:\n",
    "•\tbased on a robust estimator of covariance, which is assuming that the data are Gaussian distributed and performs better than the One-Class SVM in that case.\n",
    "•\tusing the One-Class SVM and its ability to capture the shape of the data set, hence performing better when the data is strongly non-Gaussian, i.e. with two well-separated clusters;\n",
    "•\tusing the Isolation Forest algorithm, which is based on random forests and hence more adapted to large-dimensional settings, even if it performs quite well in the examples below.\n",
    "•\tusing the Local Outlier Factor to measure the local deviation of a given data point with respect to its neighbors by comparing their local density.\n",
    "The ground truth about inliers and outliers is given by the points colors while the orange-filled area indicates which points are reported as inliers by each method.\n",
    "Here, we assume that we know the fraction of outliers in the datasets. Thus rather than using the ‘predic\n",
    "\n",
    "Example file located in: auto_examples_jupyter/Covariance/plot_outlier_detection.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/covariance/plot_outlier_detection.ipynb\n",
    "\n",
    "### Cross decomposition\n",
    "\n",
    "Instruction 5: Provide a comparison of cross decomposition methods using your dataset\n",
    "\n",
    "Simple usage of various cross decomposition algorithms: - PLSCanonical - PLSRegression, with multivariate response, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA\n",
    "Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the ‘directions of covariance’, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the scatterplot matrix display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/cross_decomposition/plot_compare_cross_decomposition.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/cross_decomposition/plot_compare_cross_decomposition.ipynb\n",
    "\n",
    "### Decomposition\n",
    "Instruction 6: Provide an example of PCA using your dataset\n",
    "\n",
    "Principal Component Analysis applied to the Iris dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/decomposition/plot_pca_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/decomposition/plot_pca_iris.ipynb\n",
    "\n",
    "Instruction 7: Provide an example of a Comparison of LDA and PCA 2D projection of your dataset\n",
    "\n",
    "The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/decomposition/plot_pca_vs_lda.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/decomposition/plot_pca_vs_lda.ipynb\n",
    "\n",
    "### Ensemble methods\n",
    "Instruction 8: Provide an example of Plotting the decision surfaces of ensembles of trees using your dataset\n",
    "\n",
    "Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/ensemble/plot_forest_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/ensemble/plot_forest_iris.ipynb\n",
    "\n",
    "### Tutorial exercises\n",
    "Instruction 9: Provide an example of SVM using your dataset\n",
    "\n",
    "A tutorial exercise for using different SVM kernels.\n",
    "This exercise is used in the Using kernels part of the Supervised learning: predicting an output variable from high-dimensional observations section of the A tutorial on statistical-learning for scientific data processing.\n",
    "plot_iris_exercise.ipynb\n",
    "Cross-validation on Digits Dataset Exercise\n",
    "A tutorial exercise using Cross-validation with an SVM on the Digits dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/exercises/plot_cv_digits.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/exercises/plot_cv_digits.ipynb\n",
    "\n",
    "### Feature Selection\n",
    "Instruction 10: Provide an example of Feature selection using SelectFromModel and LassoCV using your dataset\n",
    "\n",
    "Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.\n",
    "plot_select_from_model_boston.ipynb\n",
    "Recursive feature elimination with cross-validation\n",
    "A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/feature_selection/plot_select_from_model_boston.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/feature_selection/plot_select_from_model_boston.ipynb\n",
    "\n",
    "Instruction 11: Provide an example of Univariate Feature Selection using your dataset.\n",
    "\n",
    "An example showing univariate feature selection.\n",
    "Noisy (non informative) features are added to the iris data and univariate feature selection is applied. For each feature, we plot the p-values for the univariate feature selection and the corresponding weights of an SVM. We can see that univariate feature selection selects the informative features and that these have larger SVM weights.\n",
    "In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification\n",
    "\n",
    "Example file located in: auto_examples_jupyter/feature_selection/plot_feature_selection.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/feature_selection/plot_feature_selection.ipynb\n",
    "\n",
    "\n",
    "### Gaussian Process for Machine Learning\n",
    "Instruction 12: Provide an example of Gaussian process classification (GPC) on your dataset\n",
    "\n",
    "This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/gausian_process/plot_gpc_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/gaussian_process/plot_gpc_iris.ipynb\n",
    "\n",
    "### Generalized Linear Models\n",
    "Instruction 13: Provide an example of Plotting multi-class SGD on your dataset\n",
    "\n",
    "Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.\n",
    "plot_sgd_iris.ipynb\n",
    "Logistic Regression 3-class Classifier\n",
    "Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set>_ dataset. The datapoints are colored according to their labels.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/linear_models/plot_iris_logistic.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/linear_model/plot_iris_logistic.ipynb\n",
    "\n",
    "### Model Selection\n",
    "Instruction 14: Provide an example of Underfitting vs. Overfitting using your dataset\n",
    "\n",
    "This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/model_selection/plot_underfitting_overfitting.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/model_selection/plot_underfitting_overfitting.ipynb\n",
    "\n",
    "### Nearest Neighbors\n",
    "Instruction 15: Provide an example of Nearest Neighbors Classification using your dataset.\n",
    "\n",
    "Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/neighbors/plot_classification.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/neighbors/plot_classification.ipynb\n",
    "\n",
    "### Neural Networks\n",
    "Instruction 16: Provide an example of Varying regularization in Multi-layer Perceptron using your dataset\n",
    "\n",
    "A comparison of different values for regularization parameter ‘alpha’ on synthetic datasets. The plot shows that different alphas yield different decision functions.\n",
    "Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/neural_networks/plot_mlp_alpha.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/neural_networks/plot_mlp_alpha.ipynb\n",
    "\n",
    "### Preprocessing\n",
    "Instruction 17: Provide an example of Importance of Feature Scaling using your dataset\n",
    "\n",
    "Feature scaling though standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
    "While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\n",
    "To illustrate this, PCA is performed comparing the use of data with StandardScaler applied, to unscaled data. The results are visualized and a clear difference noted. The 1st principal component in the unscaled set can be seen. It can be seen that feature #13 dominates the direction, being a whole two orders of magnitude above the other features. This is contrasted when observing the principal component for the scaled version of the data. In the scaled version, the orders of magnitude are roughly the same across all the features.\n",
    "The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).\n",
    "The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/preprocessing/plot_scaling_importance.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/preprocessing/plot_scaling_importance.ipynb\n",
    "\n",
    "Instruction 18: Provide an example of a comparison of the effect of different scalers on your dataset with outliers\n",
    "\n",
    "Feature 0 (median income in a block) and feature 5 (number of households) of the California housing dataset have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.\n",
    "Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.\n",
    "This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.\n",
    "Scalers are linear (or more precisely affine) transformers and differ from each other in the way to estimate the parameters used to shift and scale each feature.\n",
    "QuantileTransformer provides a non-linear transformation in which distances between marginal outliers and inliers are shrunk.\n",
    "Unlike the previous transformations, normalization refers to a per sample transformation instead of a per feature transformation.\n",
    "The following code is a bit verbose, feel free to jump directly to the analysis of the results.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/preprocessing/plot_all_scaling.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/preprocessing/plot_all_scaling.ipynb\n",
    "\n",
    "### Semi Supervised Classification\n",
    "Instruction 19: Provide an example of the decision boundary of label propagation versus SVM on your dataset\n",
    "\n",
    "Comparison for decision boundary generated on iris dataset between Label Propagation and SVM.\n",
    "This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/semi_supervised/plot_label_propagation_versus_svm_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/semi_supervised/plot_label_propagation_versus_svm_iris.ipynb\n",
    "\n",
    "### Support Vector Machines\n",
    "Instruction 20: Provide an example of Plot different SVM classifiers in your dataset\n",
    "Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset:\n",
    "•\tSepal length\n",
    "•\tSepal width\n",
    "This example shows how to plot the decision surface for four SVM classifiers with different kernels.\n",
    "The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences:\n",
    "•\tLinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n",
    "•\tLinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction.\n",
    "Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/svm/plot_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/svm/plot_iris.ipynb\n",
    "\n",
    "### Decision Trees\n",
    "Instruction 21: Provide an example of plotting the decision surface of a decision tree on your dataset\n",
    "\n",
    "Plot the decision surface of a decision tree trained on pairs of features of the iris dataset.\n",
    "See decision tree for more information on the estimator.\n",
    "For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/tree/plot_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/tree/plot_iris.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import jupyter\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import requests\n",
    "import networkx as nx\n",
    "import mrjob\n",
    "import bs4\n",
    "import pattern3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>PEDS</th>\n",
       "      <th>PERSONS</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>TWAY_ID</th>\n",
       "      <th>SCH_BUS</th>\n",
       "      <th>RAIL</th>\n",
       "      <th>FATALS</th>\n",
       "      <th>DRUNK_DR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I-59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>US-SR 74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>OAKWOOD AVE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>JEFFERSON AVE SW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I-359</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>GOVERNMENT ST</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>SR-205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>SR-75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>CR-267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>SR-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>US-SR 31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>US-SR 53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I-65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CR-UNION CHAPEL ROAD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>US-SR 25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>SR-35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>CR-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>SR-169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PULASKI PIKE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CR-61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SR-53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>US-SR 53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>SR-39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>CR-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>CR-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>CR-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>CR-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>US-SR 38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>SR-133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>US-SR 6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30027</th>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>133</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>US-41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30028</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>PARK RD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30029</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>WIBU RD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30030</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>SR-20 PARKING LOT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30031</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>I-39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30032</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>SR-55 / PIONEER ST W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30033</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>THOMPSON VALLEY RD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30034</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>CR-B</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30035</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>CR-O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30036</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>CR-AA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30037</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>CHIEF CARRON RD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30038</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>US-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30039</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>I-80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30040</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>SR-92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30041</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>I-80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30042</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SR-50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30043</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>SR-487</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30044</th>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5TH ST</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30045</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30046</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>I-80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30047</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>I-80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30048</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>US-20/26/87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30049</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>I-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30050</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>US-20/SR-789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30051</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SR-113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30052</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>US-16/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30053</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>US-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30054</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I-80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30055</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I-25 SERVICE RD 0081</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30056</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>CR-1207 CHARLES AVE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30057 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATE  VE_TOTAL  PEDS  PERSONS  COUNTY  MONTH  DAY_WEEK  \\\n",
       "0          1         1     0        8     115      1         1   \n",
       "1          1         2     0        2      55      1         5   \n",
       "2          1         1     0        1      89      1         1   \n",
       "3          1         1     0        3      73      1         1   \n",
       "4          1         2     0        3     125      1         1   \n",
       "5          1         2     0        3      97      1         3   \n",
       "6          1         1     0        1      95      1         6   \n",
       "7          1         2     0        2      49      1         2   \n",
       "8          1         1     0        1      17      1         7   \n",
       "9          1         2     0        4      51      1         7   \n",
       "10         1         2     0        3       1      1         7   \n",
       "11         1         1     1        1       9      1         2   \n",
       "12         1         1     0        2      43      1         2   \n",
       "13         1         1     0        1      95      1         2   \n",
       "14         1         1     0        1     115      1         4   \n",
       "15         1         2     0        3      19      1         5   \n",
       "16         1         2     0        2       7      1         5   \n",
       "17         1         2     0        2      81      1         6   \n",
       "18         1         1     0        1      89      1         1   \n",
       "19         1         1     0        1      21      1         2   \n",
       "20         1         1     0        1      69      1         3   \n",
       "21         1         1     0        1     101      1         3   \n",
       "22         1         1     0        1      63      1         4   \n",
       "23         1         2     0        3     117      1         4   \n",
       "24         1         1     0        1      33      1         5   \n",
       "25         1         2     0        3     117      1         7   \n",
       "26         1         1     0        1      47      1         7   \n",
       "27         1         1     0        1     123      1         2   \n",
       "28         1         2     1        2      33      1         5   \n",
       "29         1         3     0        7     101      1         3   \n",
       "...      ...       ...   ...      ...     ...    ...       ...   \n",
       "30027     55        22     0       42     133     12         1   \n",
       "30028     55         1     0        1     111      9         1   \n",
       "30029     55         1     0        1      21      9         4   \n",
       "30030     55         2     0        2     101      9         6   \n",
       "30031     55         1     0        6     137      3         7   \n",
       "30032     55         1     0        2      41      6         3   \n",
       "30033     55         1     0        1     121      8         7   \n",
       "30034     55         1     0        2      63      7         4   \n",
       "30035     55         1     0        1      53     12         3   \n",
       "30036     55         1     1        1      78     11         1   \n",
       "30037     55         1     0        1      78      5         7   \n",
       "30038     55         2     0        2     123     12         3   \n",
       "30039     56         1     0        6       1      1         5   \n",
       "30040     56         1     0        6      15      1         4   \n",
       "30041     56         1     0        2       1      1         4   \n",
       "30042     56         2     0        3       5      2         3   \n",
       "30043     56         1     0        2       7      2         7   \n",
       "30044     56         3     0        3      21      2         6   \n",
       "30045     56         1     0        3      31      2         2   \n",
       "30046     56         1     0        1       1      2         7   \n",
       "30047     56         1     0        1       1      2         5   \n",
       "30048     56         2     0        4      25      3         2   \n",
       "30049     56         1     0        1       9      3         2   \n",
       "30050     56         2     0        4      17      3         1   \n",
       "30051     56         1     0        1      11      3         1   \n",
       "30052     56         2     0        3       3      3         2   \n",
       "30053     56         2     0        2      23      3         3   \n",
       "30054     56         1     0        1      37      3         6   \n",
       "30055     56         1     0        3      25      3         6   \n",
       "30056     56         2     0        1      37      3         5   \n",
       "\n",
       "                    TWAY_ID  SCH_BUS RAIL  FATALS  DRUNK_DR  \n",
       "0                      I-59        0    0       2         0  \n",
       "1                  US-SR 74        0    0       1         0  \n",
       "2               OAKWOOD AVE        0    0       1         0  \n",
       "3          JEFFERSON AVE SW        0    0       2         0  \n",
       "4                     I-359        0    0       1         0  \n",
       "5             GOVERNMENT ST        0    0       1         0  \n",
       "6                    SR-205        0    0       1         0  \n",
       "7                     SR-75        0    0       1         0  \n",
       "8                    CR-267        0    0       1         1  \n",
       "9                     SR-14        0    0       1         0  \n",
       "10                 US-SR 31        0    0       1         1  \n",
       "11                 US-SR 53        0    0       1         0  \n",
       "12                     I-65        0    0       1         0  \n",
       "13     CR-UNION CHAPEL ROAD        0    0       1         0  \n",
       "14                 US-SR 25        0    0       1         1  \n",
       "15                    SR-35        0    0       1         0  \n",
       "16                    CR-24        0    0       1         0  \n",
       "17                   SR-169        0    0       1         0  \n",
       "18             PULASKI PIKE        0    0       1         0  \n",
       "19                    CR-61        0    0       1         1  \n",
       "20                    SR-53        0    0       1         1  \n",
       "21                 US-SR 53        0    0       1         0  \n",
       "22                    SR-39        0    0       1         0  \n",
       "23                    CR-26        0    0       1         0  \n",
       "24                    CR-21        0    0       1         0  \n",
       "25                    CR-17        0    0       1         0  \n",
       "26                    CR-19        0    0       1         1  \n",
       "27                 US-SR 38        0    0       1         0  \n",
       "28                   SR-133        0    0       1         0  \n",
       "29                  US-SR 6        0    0       1         0  \n",
       "...                     ...      ...  ...     ...       ...  \n",
       "30027                 US-41        0    0       1         0  \n",
       "30028               PARK RD        0    0       1         1  \n",
       "30029               WIBU RD        0    0       1         1  \n",
       "30030     SR-20 PARKING LOT        0    0       1         0  \n",
       "30031                  I-39        0    0       3         0  \n",
       "30032  SR-55 / PIONEER ST W        0    0       1         1  \n",
       "30033    THOMPSON VALLEY RD        0    0       1         1  \n",
       "30034                  CR-B        0    0       1         0  \n",
       "30035                  CR-O        0    0       1         0  \n",
       "30036                 CR-AA        0    0       1         0  \n",
       "30037       CHIEF CARRON RD        0    0       1         1  \n",
       "30038                 US-14        0    0       1         0  \n",
       "30039                  I-80        0    0       2         0  \n",
       "30040                 SR-92        0    0       2         1  \n",
       "30041                  I-80        0    0       1         0  \n",
       "30042                 SR-50        0    0       1         0  \n",
       "30043                SR-487        0    0       1         1  \n",
       "30044                5TH ST        0    0       1         1  \n",
       "30045                  I-25        0    0       1         0  \n",
       "30046                  I-80        0    0       1         0  \n",
       "30047                  I-80        0    0       1         0  \n",
       "30048           US-20/26/87        0    0       1         0  \n",
       "30049                  I-25        0    0       1         0  \n",
       "30050          US-20/SR-789        0    0       1         1  \n",
       "30051                SR-113        0    0       1         1  \n",
       "30052              US-16/20        0    0       2         1  \n",
       "30053                 US-30        0    0       1         0  \n",
       "30054                  I-80        0    0       1         0  \n",
       "30055  I-25 SERVICE RD 0081        0    0       2         1  \n",
       "30056   CR-1207 CHARLES AVE        0    0       1         1  \n",
       "\n",
       "[30057 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Data/Fatal_Motor_Vehicle_Accidents.csv\")\n",
    "df.head(30059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-74b2f04df151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0miris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# This dataset is way too high-dimensional. Better do PCA:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\m007b\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2970\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2972\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = df\n",
    "df\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-27cdf0733686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Get data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# two clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# \"banana\"-shaped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\m007b\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2060\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2062\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\m007b\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2067\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2069\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\m007b\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1530\u001b[0m         \u001b[1;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1532\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "#Instruction 2: Provide an example of Outlier detection on your dataset.\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Virgile Fritsch <virgile.fritsch@inria.fr>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "\n",
    "# Get data\n",
    "X1 =df[:, [8, 10]]  # two clusters\n",
    "X2 =df[:, [5, 12]]  # \"banana\"-shaped\n",
    "\n",
    "# Define \"classifiers\" to be used\n",
    "classifiers = {\n",
    "    \"Empirical Covariance\": EllipticEnvelope(support_fraction=1.,\n",
    "                                             contamination=0.261),\n",
    "    \"Robust Covariance (Minimum Covariance Determinant)\":\n",
    "    EllipticEnvelope(contamination=0.261),\n",
    "    \"OCSVM\": OneClassSVM(nu=0.261, gamma=0.05)}\n",
    "colors = ['m', 'g', 'b']\n",
    "legend1 = {}\n",
    "legend2 = {}\n",
    "\n",
    "# Learn a frontier for outlier detection with several classifiers\n",
    "xx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500))\n",
    "xx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500))\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    plt.figure(1)\n",
    "    clf.fit(X1)\n",
    "    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "    Z1 = Z1.reshape(xx1.shape)\n",
    "    legend1[clf_name] = plt.contour(\n",
    "        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n",
    "    plt.figure(2)\n",
    "    clf.fit(X2)\n",
    "    Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "    Z2 = Z2.reshape(xx2.shape)\n",
    "    legend2[clf_name] = plt.contour(\n",
    "        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])\n",
    "\n",
    "legend1_values_list = list(legend1.values())\n",
    "legend1_keys_list = list(legend1.keys())\n",
    "\n",
    "# Plot the results (= shape of the data points cloud)\n",
    "plt.figure(1)  # two clusters\n",
    "plt.title(\"Outlier detection on a real data set (boston housing)\")\n",
    "plt.scatter(X1[:, 0], X1[:, 1], color='black')\n",
    "bbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"->\")\n",
    "plt.annotate(\"several confounded points\", xy=(24, 19),\n",
    "             xycoords=\"data\", textcoords=\"data\",\n",
    "             xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)\n",
    "plt.xlim((xx1.min(), xx1.max()))\n",
    "plt.ylim((yy1.min(), yy1.max()))\n",
    "plt.legend((legend1_values_list[0].collections[0],\n",
    "            legend1_values_list[1].collections[0],\n",
    "            legend1_values_list[2].collections[0]),\n",
    "           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n",
    "           loc=\"upper center\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=12))\n",
    "plt.ylabel(\"accessibility to radial highways\")\n",
    "plt.xlabel(\"pupil-teacher ratio by town\")\n",
    "\n",
    "legend2_values_list = list(legend2.values())\n",
    "legend2_keys_list = list(legend2.keys())\n",
    "\n",
    "plt.figure(2)  # \"banana\" shape\n",
    "plt.title(\"Outlier detection on a real data set (boston housing)\")\n",
    "plt.scatter(X2[:, 0], X2[:, 1], color='black')\n",
    "plt.xlim((xx2.min(), xx2.max()))\n",
    "plt.ylim((yy2.min(), yy2.max()))\n",
    "plt.legend((legend2_values_list[0].collections[0],\n",
    "            legend2_values_list[1].collections[0],\n",
    "            legend2_values_list[2].collections[0]),\n",
    "           (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n",
    "           loc=\"upper center\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=12))\n",
    "plt.ylabel(\"% lower status of the population\")\n",
    "plt.xlabel(\"average number of rooms per dwelling\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataset_fixed_cov() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5a0f448d36ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mplot_ellipse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcovariances_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'blue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_fixed_cov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_cov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;31m# Linear Discriminant Analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"svd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstore_covariance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dataset_fixed_cov() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "from matplotlib import colors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate datasets\n",
    "def dataset_fixed_cov(df):\n",
    "   \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov(df):\n",
    "    '''Generate 2 Gaussians samples with different covariance matrices'''\n",
    "  \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot functions\n",
    "def plot_data(lda, X, y, y_pred, fig_index):\n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='red', markeredgecolor='k')\n",
    "    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#990000', markeredgecolor='k')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='blue', markeredgecolor='k')\n",
    "    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#000099', markeredgecolor='k')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.))\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "    # means\n",
    "    plt.plot(lda.means_[0][0], lda.means_[0][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "    plt.plot(lda.means_[1][0], lda.means_[1][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "\n",
    "    return splot\n",
    "\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='yellow',\n",
    "                              linewidth=2, zorder=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.5)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(())\n",
    "    splot.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "\n",
    "\n",
    "def plot_qda_cov(qda, splot):\n",
    "    plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')\n",
    "    plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')\n",
    "\n",
    "for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n",
    "    # Linear Discriminant Analysis\n",
    "    lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "    y_pred = lda.fit(X, y).predict(X)\n",
    "    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n",
    "    plot_lda_cov(lda, splot)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Quadratic Discriminant Analysis\n",
    "    qda = QuadraticDiscriminantAnalysis(store_covariances=True)\n",
    "    y_pred = qda.fit(X, y).predict(X)\n",
    "    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n",
    "    plot_qda_cov(qda, splot)\n",
    "    plt.axis('tight')\n",
    "plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant'\n",
    "             'Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
