{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "\n",
    "### Due by 11:59pm on 30 November ( submit within Portfolio )\n",
    "\n",
    "For each instruction, show your code and execution within the Jupyter Notebook.  Download example files from:\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics\n",
    " - files are located in the auto_examples_jupyter directory\n",
    "\n",
    "For HW3 we will be using scikit-learn. Follow the instructions and use the examples provided within the instruction that use the Iris dataset.\n",
    "\n",
    "Once complete, upload your results to Github and update the Assignment 3 link within your portfolio's.\n",
    "\n",
    "Instruction 1: Provide an example of concatenating multiple feature extraction methods using your dataset.\n",
    "\n",
    "In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection.\n",
    "Combining features using this transformer has the benefit that it allows cross validation and grid searches over the whole process.\n",
    "The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/plot_feature_stacker.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/plot_feature_stacker.ipynb\n",
    "    \n",
    "\n",
    "### Applications\n",
    "Instruction 2: Provide an example of Outlier detection on your dataset.\n",
    "\n",
    "This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.\n",
    "We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.\n",
    "In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/applications/plot_outlier_detection_housing.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/applications/plot_outlier_detection_housing.ipynb\n",
    "\n",
    "### Classification\n",
    "Instruction 3: Provide an example of Classifier Comparison using yoru dataset\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.\n",
    "Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\n",
    "The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n",
    "auto_examples_jupyter/Classification/plot_classifier_comparison. ipynb\n",
    "Plot classification probability\n",
    "Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting, and Gaussian process classification.\n",
    "The logistic regression is not a multiclass classifier out of the box. As a result it can identify only the first class.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/auto_examples_jupyter/Classification/plot_lda_qda.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/classification/plot_lda_qda.ipynb\n",
    "\n",
    "### Clustering\n",
    "Instruction 4: Provide an example of K-means Clustering using your dataset\n",
    "\n",
    "The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/clustering/plot_cluster_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/cluster/plot_cluster_iris.ipynb\n",
    "\n",
    "\n",
    "### Covariance Estimation\n",
    "Instruction 4: Provide an example of Outlier detection with covariance estimation using your dataset.\n",
    "\n",
    "When the amount of contamination is known, this example illustrates three different ways of performing Novelty and Outlier Detection:\n",
    "•\tbased on a robust estimator of covariance, which is assuming that the data are Gaussian distributed and performs better than the One-Class SVM in that case.\n",
    "•\tusing the One-Class SVM and its ability to capture the shape of the data set, hence performing better when the data is strongly non-Gaussian, i.e. with two well-separated clusters;\n",
    "•\tusing the Isolation Forest algorithm, which is based on random forests and hence more adapted to large-dimensional settings, even if it performs quite well in the examples below.\n",
    "•\tusing the Local Outlier Factor to measure the local deviation of a given data point with respect to its neighbors by comparing their local density.\n",
    "The ground truth about inliers and outliers is given by the points colors while the orange-filled area indicates which points are reported as inliers by each method.\n",
    "Here, we assume that we know the fraction of outliers in the datasets. Thus rather than using the ‘predic\n",
    "\n",
    "Example file located in: auto_examples_jupyter/Covariance/plot_outlier_detection.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/covariance/plot_outlier_detection.ipynb\n",
    "\n",
    "### Cross decomposition\n",
    "\n",
    "Instruction 5: Provide a comparison of cross decomposition methods using your dataset\n",
    "\n",
    "Simple usage of various cross decomposition algorithms: - PLSCanonical - PLSRegression, with multivariate response, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA\n",
    "Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the ‘directions of covariance’, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the scatterplot matrix display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/cross_decomposition/plot_compare_cross_decomposition.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/cross_decomposition/plot_compare_cross_decomposition.ipynb\n",
    "\n",
    "### Decomposition\n",
    "Instruction 6: Provide an example of PCA using your dataset\n",
    "\n",
    "Principal Component Analysis applied to the Iris dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/decomposition/plot_pca_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/decomposition/plot_pca_iris.ipynb\n",
    "\n",
    "Instruction 7: Provide an example of a Comparison of LDA and PCA 2D projection of your dataset\n",
    "\n",
    "The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/decomposition/plot_pca_vs_lda.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/decomposition/plot_pca_vs_lda.ipynb\n",
    "\n",
    "### Ensemble methods\n",
    "Instruction 8: Provide an example of Plotting the decision surfaces of ensembles of trees using your dataset\n",
    "\n",
    "Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/ensemble/plot_forest_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/ensemble/plot_forest_iris.ipynb\n",
    "\n",
    "### Tutorial exercises\n",
    "Instruction 9: Provide an example of SVM using your dataset\n",
    "\n",
    "A tutorial exercise for using different SVM kernels.\n",
    "This exercise is used in the Using kernels part of the Supervised learning: predicting an output variable from high-dimensional observations section of the A tutorial on statistical-learning for scientific data processing.\n",
    "plot_iris_exercise.ipynb\n",
    "Cross-validation on Digits Dataset Exercise\n",
    "A tutorial exercise using Cross-validation with an SVM on the Digits dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/exercises/plot_cv_digits.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/exercises/plot_cv_digits.ipynb\n",
    "\n",
    "### Feature Selection\n",
    "Instruction 10: Provide an example of Feature selection using SelectFromModel and LassoCV using your dataset\n",
    "\n",
    "Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.\n",
    "plot_select_from_model_boston.ipynb\n",
    "Recursive feature elimination with cross-validation\n",
    "A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/feature_selection/plot_select_from_model_boston.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/feature_selection/plot_select_from_model_boston.ipynb\n",
    "\n",
    "Instruction 11: Provide an example of Univariate Feature Selection using your dataset.\n",
    "\n",
    "An example showing univariate feature selection.\n",
    "Noisy (non informative) features are added to the iris data and univariate feature selection is applied. For each feature, we plot the p-values for the univariate feature selection and the corresponding weights of an SVM. We can see that univariate feature selection selects the informative features and that these have larger SVM weights.\n",
    "In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification\n",
    "\n",
    "Example file located in: auto_examples_jupyter/feature_selection/plot_feature_selection.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/feature_selection/plot_feature_selection.ipynb\n",
    "\n",
    "\n",
    "### Gaussian Process for Machine Learning\n",
    "Instruction 12: Provide an example of Gaussian process classification (GPC) on your dataset\n",
    "\n",
    "This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/gausian_process/plot_gpc_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/gaussian_process/plot_gpc_iris.ipynb\n",
    "\n",
    "### Generalized Linear Models\n",
    "Instruction 13: Provide an example of Plotting multi-class SGD on your dataset\n",
    "\n",
    "Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.\n",
    "plot_sgd_iris.ipynb\n",
    "Logistic Regression 3-class Classifier\n",
    "Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set>_ dataset. The datapoints are colored according to their labels.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/linear_models/plot_iris_logistic.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/linear_model/plot_iris_logistic.ipynb\n",
    "\n",
    "### Model Selection\n",
    "Instruction 14: Provide an example of Underfitting vs. Overfitting using your dataset\n",
    "\n",
    "This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/model_selection/plot_underfitting_overfitting.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/model_selection/plot_underfitting_overfitting.ipynb\n",
    "\n",
    "### Nearest Neighbors\n",
    "Instruction 15: Provide an example of Nearest Neighbors Classification using your dataset.\n",
    "\n",
    "Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/neighbors/plot_classification.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/neighbors/plot_classification.ipynb\n",
    "\n",
    "### Neural Networks\n",
    "Instruction 16: Provide an example of Varying regularization in Multi-layer Perceptron using your dataset\n",
    "\n",
    "A comparison of different values for regularization parameter ‘alpha’ on synthetic datasets. The plot shows that different alphas yield different decision functions.\n",
    "Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/neural_networks/plot_mlp_alpha.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/neural_networks/plot_mlp_alpha.ipynb\n",
    "\n",
    "### Preprocessing\n",
    "Instruction 17: Provide an example of Importance of Feature Scaling using your dataset\n",
    "\n",
    "Feature scaling though standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
    "While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\n",
    "To illustrate this, PCA is performed comparing the use of data with StandardScaler applied, to unscaled data. The results are visualized and a clear difference noted. The 1st principal component in the unscaled set can be seen. It can be seen that feature #13 dominates the direction, being a whole two orders of magnitude above the other features. This is contrasted when observing the principal component for the scaled version of the data. In the scaled version, the orders of magnitude are roughly the same across all the features.\n",
    "The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).\n",
    "The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/preprocessing/plot_scaling_importance.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/preprocessing/plot_scaling_importance.ipynb\n",
    "\n",
    "Instruction 18: Provide an example of a comparison of the effect of different scalers on your dataset with outliers\n",
    "\n",
    "Feature 0 (median income in a block) and feature 5 (number of households) of the California housing dataset have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.\n",
    "Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.\n",
    "This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.\n",
    "Scalers are linear (or more precisely affine) transformers and differ from each other in the way to estimate the parameters used to shift and scale each feature.\n",
    "QuantileTransformer provides a non-linear transformation in which distances between marginal outliers and inliers are shrunk.\n",
    "Unlike the previous transformations, normalization refers to a per sample transformation instead of a per feature transformation.\n",
    "The following code is a bit verbose, feel free to jump directly to the analysis of the results.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/preprocessing/plot_all_scaling.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/preprocessing/plot_all_scaling.ipynb\n",
    "\n",
    "### Semi Supervised Classification\n",
    "Instruction 19: Provide an example of the decision boundary of label propagation versus SVM on your dataset\n",
    "\n",
    "Comparison for decision boundary generated on iris dataset between Label Propagation and SVM.\n",
    "This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/semi_supervised/plot_label_propagation_versus_svm_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/semi_supervised/plot_label_propagation_versus_svm_iris.ipynb\n",
    "\n",
    "### Support Vector Machines\n",
    "Instruction 20: Provide an example of Plot different SVM classifiers in your dataset\n",
    "Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset:\n",
    "•\tSepal length\n",
    "•\tSepal width\n",
    "This example shows how to plot the decision surface for four SVM classifiers with different kernels.\n",
    "The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences:\n",
    "•\tLinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n",
    "•\tLinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction.\n",
    "Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/svm/plot_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/svm/plot_iris.ipynb\n",
    "\n",
    "### Decision Trees\n",
    "Instruction 21: Provide an example of plotting the decision surface of a decision tree on your dataset\n",
    "\n",
    "Plot the decision surface of a decision tree trained on pairs of features of the iris dataset.\n",
    "See decision tree for more information on the estimator.\n",
    "For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/tree/plot_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/tree/plot_iris.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import jupyter\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import requests\n",
    "import networkx as nx\n",
    "import mrjob\n",
    "import bs4\n",
    "import pattern3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Due to the limitations to processing power and the size of my data set, \n",
    "#I was forced to reduce my rows from some 33,000 down to just 1000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instruction 1: Provide an example of concatenating multiple feature extraction methods using your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1 \n",
      "[CV]  features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1, score=0.776119, total=   0.6s\n",
      "[CV] features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1, score=0.813253, total=   4.9s\n",
      "[CV] features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__pca__n_components=1, features__univ_select__k=1, svm__C=0.1, score=0.822289, total=   3.4s\n",
      "[CV] features__pca__n_components=1, features__univ_select__k=1, svm__C=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    9.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__pca__n_components=1, features__univ_select__k=1, svm__C=1, score=0.776119, total=  46.7s\n",
      "[CV] features__pca__n_components=1, features__univ_select__k=1, svm__C=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   56.1s remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")\n",
    "\n",
    "X,y = data[0:, 0:8], data[:,9] \n",
    "\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instruction 2: Provide an example of Outlier detection on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")\n",
    "\n",
    "X1 = load_data()['data'][:, [8, 9]]  # two clusters\n",
    "X2 = load_data()['data'][:, [5, 12]]  # \"banana\"-shaped\n",
    "\n",
    "# Define \"classifiers\" to be used\n",
    "classifiers = {\n",
    "    \"Empirical Covariance\": EllipticEnvelope(support_fraction=1.,\n",
    "                                             contamination=0.261),\n",
    "    \"Robust Covariance (Minimum Covariance Determinant)\":\n",
    "    EllipticEnvelope(contamination=0.261),\n",
    "    \"OCSVM\": OneClassSVM(nu=0.261, gamma=0.05)}\n",
    "colors = ['m', 'g', 'b']\n",
    "legend1 = {}\n",
    "legend2 = {}\n",
    "\n",
    "# Learn a frontier for outlier detection with several classifiers\n",
    "xx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500))\n",
    "xx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500))\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    plt.figure(1)\n",
    "    clf.fit(X1)\n",
    "    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "    Z1 = Z1.reshape(xx1.shape)\n",
    "    legend1[clf_name] = plt.contour(\n",
    "        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n",
    "    plt.figure(2)\n",
    "    clf.fit(X2)\n",
    "    Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "    Z2 = Z2.reshape(xx2.shape)\n",
    "    legend2[clf_name] = plt.contour(\n",
    "        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])\n",
    "\n",
    "legend1_values_list = list(legend1.values())\n",
    "legend1_keys_list = list(legend1.keys())\n",
    "\n",
    "# Plot the results (= shape of the data points cloud)\n",
    "plt.figure(1)  # two clusters\n",
    "plt.title(\"Outlier detection on a real data set (boston housing)\")\n",
    "plt.scatter(X1[:, 0], X1[:, 1], color='black')\n",
    "bbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"->\")\n",
    "plt.annotate(\"several confounded points\", xy=(24, 19),\n",
    "             xycoords=\"data\", textcoords=\"data\",\n",
    "             xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)\n",
    "plt.xlim((xx1.min(), xx1.max()))\n",
    "plt.ylim((yy1.min(), yy1.max()))\n",
    "plt.legend((legend1_values_list[0].collections[0],\n",
    "            legend1_values_list[1].collections[0],\n",
    "            legend1_values_list[2].collections[0]),\n",
    "           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n",
    "           loc=\"upper center\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=12))\n",
    "plt.ylabel(\"accessibility to radial highways\")\n",
    "plt.xlabel(\"pupil-teacher ratio by town\")\n",
    "\n",
    "legend2_values_list = list(legend2.values())\n",
    "legend2_keys_list = list(legend2.keys())\n",
    "\n",
    "plt.figure(2)  # \"banana\" shape\n",
    "plt.title(\"Outlier detection on a real data set (boston housing)\")\n",
    "plt.scatter(X2[:, 0], X2[:, 1], color='black')\n",
    "plt.xlim((xx2.min(), xx2.max()))\n",
    "plt.ylim((yy2.min(), yy2.max()))\n",
    "plt.legend((legend2_values_list[0].collections[0],\n",
    "            legend2_values_list[1].collections[0],\n",
    "            legend2_values_list[2].collections[0]),\n",
    "           (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n",
    "           loc=\"upper center\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=12))\n",
    "plt.ylabel(\"% lower status of the population\")\n",
    "plt.xlabel(\"average number of rooms per dwelling\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 3: Provide an example of Classifier Comparison using yoru dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate datasets\n",
    "def dataset_fixed_cov():\n",
    "    '''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -0.23], [0.83, .23]])\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov():\n",
    "    '''Generate 2 Gaussians samples with different covariance matrices'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot functions\n",
    "def plot_data(lda, X, y, y_pred, fig_index):\n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='red', markeredgecolor='k')\n",
    "    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#990000', markeredgecolor='k')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='blue', markeredgecolor='k')\n",
    "    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#000099', markeredgecolor='k')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.))\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "    # means\n",
    "    plt.plot(lda.means_[0][0], lda.means_[0][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "    plt.plot(lda.means_[1][0], lda.means_[1][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "\n",
    "    return splot\n",
    "\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='yellow',\n",
    "                              linewidth=2, zorder=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.5)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(())\n",
    "    splot.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "\n",
    "\n",
    "def plot_qda_cov(qda, splot):\n",
    "    plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')\n",
    "    plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')\n",
    "\n",
    "for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n",
    "    # Linear Discriminant Analysis\n",
    "    lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "    y_pred = lda.fit(X, y).predict(X)\n",
    "    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n",
    "    plot_lda_cov(lda, splot)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Quadratic Discriminant Analysis\n",
    "    qda = QuadraticDiscriminantAnalysis(store_covariances=True)\n",
    "    y_pred = qda.fit(X, y).predict(X)\n",
    "    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n",
    "    plot_qda_cov(qda, splot)\n",
    "    plt.axis('tight')\n",
    "plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant'\n",
    "             'Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 4: Provide an example of K-means Clustering using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 4: Provide an example of Outlier detection with covariance estimation using your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import sklearn.neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Example settings\n",
    "n_samples = 200\n",
    "outliers_fraction = 0.25\n",
    "clusters_separation = [0, 1, 2]\n",
    "\n",
    "# define two outlier detection tools to be compared\n",
    "classifiers = {\n",
    "    \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n",
    "                                     kernel=\"rbf\", gamma=0.1),\n",
    "    \"Robust covariance\": EllipticEnvelope(contamination=outliers_fraction),\n",
    "    \"Isolation Forest\": IsolationForest(max_samples=n_samples,\n",
    "                                        contamination=outliers_fraction,\n",
    "                                        random_state=rng),\n",
    "    \"Local Outlier Factor\": LocalOutlierFactor(\n",
    "        n_neighbors=35,\n",
    "        contamination=outliers_fraction)}\n",
    "\n",
    "# Compare given classifiers under given settings\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\n",
    "n_inliers = int((1. - outliers_fraction) * n_samples)\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "ground_truth = np.ones(n_samples, dtype=int)\n",
    "ground_truth[-n_outliers:] = -1\n",
    "\n",
    "# Fit the problem with varying cluster separation\n",
    "for i, offset in enumerate(clusters_separation):\n",
    "    np.random.seed(42)\n",
    "    # Data generation\n",
    "    X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset\n",
    "    X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset\n",
    "    X = np.r_[X1, X2]\n",
    "    # Add outliers\n",
    "    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]\n",
    "\n",
    "    # Fit the model\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        # fit the data and tag outliers\n",
    "        if clf_name == \"Local Outlier Factor\":\n",
    "            y_pred = clf.fit_predict(X)\n",
    "            scores_pred = clf.negative_outlier_factor_\n",
    "        else:\n",
    "            clf.fit(X)\n",
    "            scores_pred = clf.decision_function(X)\n",
    "            y_pred = clf.predict(X)\n",
    "        threshold = stats.scoreatpercentile(scores_pred,\n",
    "                                            100 * outliers_fraction)\n",
    "        n_errors = (y_pred != ground_truth).sum()\n",
    "        # plot the levels lines and the points\n",
    "        if clf_name == \"Local Outlier Factor\":\n",
    "            # decision_function is private for LOF\n",
    "            Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        subplot = plt.subplot(2, 2, i + 1)\n",
    "        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "                         cmap=plt.cm.Blues_r)\n",
    "        a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                            linewidths=2, colors='red')\n",
    "        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                         colors='orange')\n",
    "        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',\n",
    "                            s=20, edgecolor='k')\n",
    "        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',\n",
    "                            s=20, edgecolor='k')\n",
    "        subplot.axis('tight')\n",
    "        subplot.legend(\n",
    "            [a.collections[0], b, c],\n",
    "            ['learned decision function', 'true inliers', 'true outliers'],\n",
    "            prop=matplotlib.font_manager.FontProperties(size=10),\n",
    "            loc='lower right')\n",
    "        subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n",
    "        subplot.set_xlim((-7, 7))\n",
    "        subplot.set_ylim((-7, 7))\n",
    "    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 5: Provide a comparison of cross decomposition methods using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 6: Provide an example of PCA using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 7: Provide an example of a Comparison of LDA and PCA 2D projection of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")                      \n",
    "X,y = data[0:, 0:8], data[:,9] \n",
    "\n",
    "state_target_names = np.array(['New York', 'Ohio', 'Michigan', \"Wisconson\"], dtype='|S10')\n",
    "target_names = state_target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['navy', 'turquoise', 'darkorange', 'red']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2, 3], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of Vehical Fatality dataset')\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2, 3], target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of Vehical Fatality dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 8: Provide an example of Plotting the decision surfaces of ensembles of trees using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "n_estimators = 30\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "\n",
    "plot_idx = 1\n",
    "\n",
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators),\n",
    "          ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                             n_estimators=n_estimators)]\n",
    "\n",
    "for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X,y = data[:, pair], data[:,9] \n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Standardize\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X = (X - mean) / std\n",
    "\n",
    "        # Train\n",
    "        clf = clone(model)\n",
    "        clf = model.fit(X, y)\n",
    "\n",
    "        scores = clf.score(X, y)\n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\n",
    "            \".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(\n",
    "                len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair,\n",
    "              \"has a score of\", scores)\n",
    "\n",
    "        plt.subplot(3, 4, plot_idx)\n",
    "        if plot_idx <= len(models):\n",
    "            # Add a title at the top of each column\n",
    "            plt.title(model_title)\n",
    "\n",
    "        # Now plot the decision boundary using a fine mesh as input to a\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifiers\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "            # Choose alpha blend level with respect to the number\n",
    "            # of estimators\n",
    "            # that are in use (noting that AdaBoost can use fewer estimators\n",
    "            # than its maximum if it achieves a good enough fit early on)\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "        # Build a coarser grid to plot a set of ensemble classifications\n",
    "        # to show how these are different to what we see in the decision\n",
    "        # surfaces. These points are regularly space and do not have a\n",
    "        # black outline\n",
    "        xx_coarser, yy_coarser = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step_coarser),\n",
    "            np.arange(y_min, y_max, plot_step_coarser))\n",
    "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                         yy_coarser.ravel()]\n",
    "                                         ).reshape(xx_coarser.shape)\n",
    "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                                c=Z_points_coarser, cmap=cmap,\n",
    "                                edgecolors=\"none\")\n",
    "\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
    "                    edgecolor='k', s=20)\n",
    "        plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\")\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 9: Provide an example of SVM using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "\n",
    "X,y = data[0:, 0:8], data[:,9] \n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "C_s = np.logspace(-100, 0, 100)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "for C in C_s:\n",
    "    svc.C = C\n",
    "    this_scores = cross_val_score(svc, X, y, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "# Do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.semilogx(C_s, scores)\n",
    "plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 10: Provide an example of Feature selection using SelectFromModel and LassoCV using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Load the boston dataset.\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "\n",
    "X,y = data[0:, 0:8], data[:,9] \n",
    "\n",
    "# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "clf = LassoCV()\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(clf, threshold=0.25)\n",
    "sfm.fit(X, y)\n",
    "n_features = sfm.transform(X).shape[1]\n",
    "\n",
    "# Reset the threshold till the number of features equals two.\n",
    "# Note that the attribute can be set directly instead of repeatedly\n",
    "# fitting the metatransformer.\n",
    "while n_features > 2:\n",
    "    sfm.threshold += 0.1\n",
    "    X_transform = sfm.transform(X)\n",
    "    n_features = X_transform.shape[1]\n",
    "\n",
    "# Plot the selected two features from X.\n",
    "plt.title(\n",
    "    \"Features selected from Boston using SelectFromModel with \"\n",
    "    \"threshold %0.3f.\" % sfm.threshold)\n",
    "feature1 = X_transform[:, 0]\n",
    "feature2 = X_transform[:, 1] \n",
    "plt.plot(feature1, feature2, 'r.')\n",
    "plt.xlabel(\"Feature number 1\")\n",
    "plt.ylabel(\"Feature number 2\")\n",
    "plt.ylim([np.min(feature2), np.max(feature2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 11: Provide an example of Univariate Feature Selection using your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "\n",
    "# The iris dataset\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "\n",
    "\n",
    "\n",
    "# Some noisy data not correlated\n",
    "E = np.random.uniform(0, 0.1, size=(len(data), 20))\n",
    "\n",
    "# Add the noisy data to the informative features\n",
    "X = np.hstack((data, E))\n",
    "y = data[:,9]\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "# #############################################################################\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(X, y)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "plt.bar(X_indices - .45, scores, width=.2,\n",
    "        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',\n",
    "        edgecolor='black')\n",
    "\n",
    "# #############################################################################\n",
    "# Compare to the weights of an SVM\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "svm_weights = (clf.coef_ ** 2).sum(axis=0)\n",
    "svm_weights /= svm_weights.max()\n",
    "\n",
    "plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight',\n",
    "        color='navy', edgecolor='black')\n",
    "\n",
    "clf_selected = svm.SVC(kernel='linear')\n",
    "clf_selected.fit(selector.transform(X), y)\n",
    "\n",
    "svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\n",
    "svm_weights_selected /= svm_weights_selected.max()\n",
    "\n",
    "plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n",
    "        width=.2, label='SVM weights after selection', color='c',\n",
    "        edgecolor='black')\n",
    "\n",
    "\n",
    "plt.title(\"Comparing feature selection\")\n",
    "plt.xlabel('Feature number')\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 12: Provide an example of Gaussian process classification (GPC) on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# import some data to play with\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "X = data[:, :2]  # we only take the first two features.\n",
    "y = np.array(data[:,9], dtype=int)\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "kernel = 1.0 * RBF([1.0])\n",
    "gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n",
    "kernel = 1.0 * RBF([1.0, 1.0])\n",
    "gpc_rbf_anisotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "titles = [\"Isotropic RBF\", \"Anisotropic RBF\"]\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):\n",
    "    # Plot the predicted probabilities. For that, we will assign a color to\n",
    "    # each point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n",
    "    plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin=\"lower\")\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.array([\"r\", \"g\", \"b\"])[y],\n",
    "                edgecolors=(0, 0, 0))\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(\"%s, LML: %.3f\" %\n",
    "              (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 13: Provide an example of Plotting multi-class SGD on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "\n",
    "X = data[:, :2]\n",
    "Y = data[:,9]\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Persona')\n",
    "plt.ylabel('State')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 14: Provide an example of Underfitting vs. Overfitting using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 15: Provide an example of Nearest Neighbors Classification using your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")   \n",
    "\n",
    "X = data[:, :2]\n",
    "y = data[:,9]\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 16: Provide an example of Varying regularization in Multi-layer Perceptron using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Author: Issam H. Laradji\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "alphas = np.logspace(-5, 3, 5)\n",
    "names = []\n",
    "for i in alphas:\n",
    "    names.append('alpha ' + str(i))\n",
    "\n",
    "classifiers = []\n",
    "for i in alphas:\n",
    "    classifiers.append(MLPClassifier(alpha=i, random_state=1))\n",
    "np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\") \n",
    "X = data[:, :8]\n",
    "y = data[:,9]\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable]\n",
    "\n",
    "figure = plt.figure(figsize=(17, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for X, y in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='black', s=25)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6, edgecolors='black', s=25)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 17: Provide an example of Importance of Feature Scaling using your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "print(__doc__)\n",
    "\n",
    "# Code source: Tyler Lanigan <tylerlanigan@gmail.com>\n",
    "#              Sebastian Raschka <mail@sebastianraschka.com>\n",
    "\n",
    "# License: BSD 3 clause\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "FIG_SIZE = (10, 7)\n",
    "\n",
    "\n",
    "features, target = load_wine(return_X_y=True)\n",
    "\n",
    "# Make a train/test split using 30% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "# Fit to data and predict using pipelined GNB and PCA.\n",
    "unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n",
    "unscaled_clf.fit(X_train, y_train)\n",
    "pred_test = unscaled_clf.predict(X_test)\n",
    "\n",
    "# Fit to data and predict using pipelined scaling, GNB and PCA.\n",
    "std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\n",
    "std_clf.fit(X_train, y_train)\n",
    "pred_test_std = std_clf.predict(X_test)\n",
    "\n",
    "# Show prediction accuracies in scaled and unscaled data.\n",
    "print('\\nPrediction accuracy for the normal test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n",
    "\n",
    "print('\\nPrediction accuracy for the standardized test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n",
    "# Extract PCA from pipeline\n",
    "pca = unscaled_clf.named_steps['pca']\n",
    "pca_std = std_clf.named_steps['pca']\n",
    "\n",
    "# Show first principal componenets\n",
    "print('\\nPC 1 without scaling:\\n', pca.components_[0])\n",
    "print('\\nPC 1 with scaling:\\n', pca_std.components_[0])\n",
    "\n",
    "# Scale and use PCA on X_train data for visualization.\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "X_train_std = pca_std.transform(scaler.transform(X_train))\n",
    "\n",
    "# visualize standardized vs. untouched dataset with PCA performed\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n",
    "\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train[y_train == l, 0], X_train[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std[y_train == l, 0], X_train_std[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "ax1.set_title('Training dataset after PCA')\n",
    "ax2.set_title('Standardized training dataset after PCA')\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Instruction 18: Provide an example of a comparison of the effect of different scalers on your dataset with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "\n",
    "# Take only 2 features to make visualization easier\n",
    "# Feature of 0 has a long tail distribution.\n",
    "# Feature 5 has a few but very large outliers.\n",
    "\n",
    "X = X_full[:, [0, 5]]\n",
    "\n",
    "distributions = [\n",
    "    ('Unscaled data', X),\n",
    "    ('Data after standard scaling',\n",
    "        StandardScaler().fit_transform(X)),\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler().fit_transform(X)),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler().fit_transform(X)),\n",
    "    ('Data after robust scaling',\n",
    "        RobustScaler(quantile_range=(25, 75)).fit_transform(X)),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        .fit_transform(X)),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        .fit_transform(X)),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer().fit_transform(X))\n",
    "]\n",
    "\n",
    "# scale the output between 0 and 1 for the colorbar\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "\n",
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return ((ax_scatter, ax_histy, ax_histx),\n",
    "            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "            ax_colorbar)\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\",\n",
    "                      x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cm.plasma_r(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker='o', s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation='horizontal',\n",
    "                 color='grey', ec='grey')\n",
    "    hist_X1.axis('off')\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation='vertical',\n",
    "                 color='grey', ec='grey')\n",
    "    hist_X0.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction 19: Provide an example of the decision boundary of label propagation versus SVM on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction 20: Provide an example of Plot different SVM classifiers in your dataset Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset: • Sepal length • Sepal width This example shows how to plot the decision surface for four SVM classifiers with different kernels. The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences: • LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss. • LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction. Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction 21: Provide an example of plotting the decision surface of a decision tree on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 4\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(\"Fatal_Motor_Vehicle_Accidents.txt\")                       # myfile.txt contains 4 columns of numbers\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = data[:, pair]\n",
    "    y = data[:,9]\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
